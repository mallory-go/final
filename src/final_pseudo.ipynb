{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10217485",
   "metadata": {},
   "source": [
    "Title \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b41e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "import shap\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "df = pd.read_csv('/Users/mallorygo/Desktop/DATA1030/data1030-fall2025/final reports/ufc-master.csv')\n",
    "pd.set_option(\"display.max_columns\", None, \"display.max_rows\", None)\n",
    "\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed2c360",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None, \"display.max_rows\", None)\n",
    "\n",
    "print(df.dtypes) \n",
    "# How many rows and columns do we have in df_merge?\n",
    "print(df.shape[0]) # number of rows\n",
    "print(df.shape[1]) # number of rows\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59da5783",
   "metadata": {},
   "outputs": [],
   "source": [
    "comm_drop = [\n",
    "'Date','Location'\n",
    "]\n",
    "df.drop(comm_drop, axis=1, inplace = True)\n",
    "\n",
    "\n",
    "print(df.dtypes) \n",
    "print(df.shape[0]) # number of rows\n",
    "print(df.shape[1]) # number of rows\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b02a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating the features based on their data types\n",
    "cat_col = [col for col in df.columns if df[col].dtypes == 'object']\n",
    "num_col = [col for col in df.columns if col not in cat_col]\n",
    "print(df['Winner'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1176e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#take care of missingness in Winner\n",
    "print(\"Unique values in Winner column:\", df['Winner'].unique())\n",
    "# Strip spaces and lowercase all values\n",
    "df['Winner'] = df['Winner'].astype(str).str.strip().str.capitalize()\n",
    "print(\"Cleaned Winner values:\", df['Winner'].unique())\n",
    "df = df[df['Winner'].isin(['Red', 'Blue'])]\n",
    "print(\"Rows remaining after filtering:\", len(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15239e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_categorieswc = df['WeightClass'].unique()\n",
    "print(unique_categorieswc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212d90ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "print(df['Winner'].value_counts(dropna=False))\n",
    "# Convert 'Red' and 'Blue' winner string to binary label (1 = Red wins, 0 = Blue wins)\n",
    "df = df[df['Winner'].isin(['Red', 'Blue'])]  # filter out 'Draw' or 'No Contest'\n",
    "print(\"Rows remaining after filtering:\", len(df))\n",
    "\n",
    "cat_col = ['RedFighter', 'BlueFighter', 'WeightClass', 'Gender']\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "enc = LabelEncoder()\n",
    "\n",
    "for col in cat_col:\n",
    "    if col in df.columns:\n",
    "        df[col] = enc.fit_transform(df[col].astype(str))\n",
    "    else:\n",
    "        print(f\"Warning: Column {col} not found in dataframe.\")\n",
    "\n",
    "print(df['Winner'].value_counts())\n",
    "#Series([], Name: count, dtype: int64)\n",
    "\n",
    "# result = \n",
    "# Series([], Name: count, dtype: int64)\n",
    "# Rows remaining after filtering: 0\n",
    "# Series([], Name: count, dtype: int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfeb231",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "win_counts = df['Winner'].value_counts()\n",
    "# Display the frequency table\n",
    "print(\"Frequency Table for 'Winner' column:\")\n",
    "print(win_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcdd3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    if df[column].isnull().sum()!=0:\n",
    "        print(f\"Nan in {column}: {df[column].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc98998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#force numeric WeightClass \n",
    "df['WeightClass'] = pd.to_numeric(df['WeightClass'], errors='coerce')\n",
    "\n",
    "weightclass_labels = {\n",
    "    0: 'Flyweight',\n",
    "    1: 'Welterweight',\n",
    "    2: 'Heavyweight',\n",
    "    3: 'Featherweight',\n",
    "    4: 'Light Heavyweight',\n",
    "    5: 'Catch Weight',\n",
    "    6: 'Lightweight',\n",
    "    7: 'Bantamweight',\n",
    "    8: \"Women's Strawweight\",\n",
    "    9: \"Women's Flyweight\",\n",
    "    10: \"Middleweight\",  \n",
    "    11: \"Women's Bantamweight\",     \n",
    "    12: \"Women's Featherweight \"               \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "df['WeightClassLabel'] = df['WeightClass'].map(weightclass_labels)\n",
    "\n",
    "#check\n",
    "missing_wtclass = df['WeightClass'].isnull().sum()\n",
    "print(f\"✅ WeightClass mapped. Missing values: {missing_wtclass}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdbb80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic descriptive statistics\n",
    "print(df.describe())\n",
    "\n",
    "# Check class balance\n",
    "sns.countplot(x='Winner', data=df)\n",
    "plt.title('Fight Winner Distribution (Red = 1, Blue = 0)')\n",
    "plt.show()\n",
    "\n",
    "# Missing data heatmap\n",
    "sns.heatmap(df.isnull(), cbar=False)\n",
    "plt.title(\"Missing Data Heatmap\")\n",
    "plt.show()\n",
    "print(df[\"BlueHeightCms\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe30f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Unique values in WeightClass column:\", df['WeightClass'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b562c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df[\"BlueHeightCms\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff14d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df[\"BlueHeight\"] = df[\"BlueHeightCms\"] / 2.54\n",
    "df['BlueReach']= df['BlueReachCms'] / 2.54\n",
    "df['RedHeight'] = df['RedHeightCms'] / 2.54\n",
    "df['RedReach']= df['RedReachCms'] / 2.54\n",
    "df.drop(columns = ['BlueHeightCms', 'BlueReachCms', 'RedHeightCms', 'RedReachCms'], axis = 1, inplace = True)\n",
    "\n",
    "df['ROver35'] = df['RedAge'].apply(lambda x: int(x > 35))\n",
    "df['BOver35'] = df['BlueAge'].apply(lambda x: int(x > 35))\n",
    "\n",
    "df['RWinPct'] = df['RedWins'] / (df['RedWins'] + df['RedLosses'])\n",
    "df['BWinPct'] = df['BlueWins'] / (df['BlueWins'] + df['BlueLosses'])\n",
    "\n",
    "df['RedStrikingRatio'] = df['RedAvgSigStrLanded'] / (df['RedAvgSigStrLanded'] + df['BlueAvgSigStrLanded'])\n",
    "df['BlueStrikingRatio'] = df['BlueAvgSigStrLanded'] / (df['RedAvgSigStrLanded'] + df['BlueAvgSigStrLanded'])\n",
    "\n",
    "df['RedTotalFights'] = df['RedWins'] + df['RedLosses']\n",
    "df['BlueTotalFights'] = df['BlueWins'] + df['BlueLosses']\n",
    "\n",
    "df['RedIsGrappler'] = (df['RedAvgTDLanded'] > 3.5).astype(int)\n",
    "df['BlueIsGrappler'] = (df['BlueAvgTDLanded'] > 3.5).astype(int)\n",
    "\n",
    "df['RedIsStriker'] = (df['RedAvgSigStrLanded'] > 6.0).astype(int)\n",
    "df['BlueIsStriker'] = (df['BlueAvgSigStrLanded'] > 6.0).astype(int)\n",
    "\n",
    "df['RGrapplerVBStriker'] = ((df['RedIsGrappler'] == 1) & (df['BlueIsStriker'] == 1)).astype(int)\n",
    "df['BGrapplerVRStriker'] = ((df['BlueIsGrappler'] == 1) & (df['RedIsStriker'] == 1)).astype(int)\n",
    "\n",
    "df['RedStrikingEfficiency'] = df['RedAvgSigStrLanded'] * df['RedAvgSigStrPct']\n",
    "df['BlueStrikingEfficiency'] = df['BlueAvgSigStrLanded'] * df['BlueAvgSigStrPct']\n",
    "\n",
    "df['RedTDEfficiency'] = df['RedAvgTDLanded'] * df['RedAvgTDPct']\n",
    "df['BlueTDEfficiency'] = df['BlueAvgTDLanded'] * df['BlueAvgTDPct']\n",
    "\n",
    "df['RedEffectiveTD'] = 1 / (df['RedAvgTDLanded'] / df['RedAvgSubAtt'])\n",
    "df['BlueEffectiveTD'] = 1 / (df['BlueAvgTDLanded'] / df['BlueAvgSubAtt'])\n",
    "df['EffectiveTDDif'] = df['BlueEffectiveTD'] - df['RedEffectiveTD']\n",
    "\n",
    "df['RedSize'] = df['RedHeight'] + df['RedReach']\n",
    "df['BlueSize'] = df['BlueHeight'] + df['BlueReach']\n",
    "\n",
    "df['Favorite'] = (df['RedOdds'] < df['BlueOdds']).astype(int)\n",
    "\n",
    "df['Favorite'] = df['Favorite'].map({0 : 'Blue', 1 : 'Red'})\n",
    "\n",
    "df['FavoriteWins'] = (df['Favorite'] == df['Winner']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe3cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f433646",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['draw_diff'] = (df['BlueDraws']-df['RedDraws'])\n",
    "df['avg_sig_str_pct_diff'] = (df['BlueAvgSigStrPct']-df['RedAvgSigStrPct'])\n",
    "df['avg_TD_pct_diff'] = (df['BlueAvgTDPct']-df['RedAvgTDPct'])\n",
    "df['win_by_Decision_Majority_diff'] = (df['BlueWinsByDecisionMajority']-df['RedWinsByDecisionMajority'])\n",
    "df['win_by_Decision_Split_diff'] = (df['BlueWinsByDecisionSplit']-df['RedWinsByDecisionSplit'])\n",
    "df['win_by_Decision_Unanimous_diff'] = (df['BlueWinsByDecisionUnanimous']-df['RedWinsByDecisionUnanimous'])\n",
    "df['win_by_TKO_Doctor_Stoppage_diff'] = (df['BlueWinsByTKODoctorStoppage']-df['RedWinsByTKODoctorStoppage'])\n",
    "df['odds_diff'] = (df['BlueOdds']-df['RedOdds'])\n",
    "df['ev_diff'] = (df['BlueExpectedValue']-df['RedExpectedValue'])\n",
    "\n",
    "\n",
    "blue_win = df[df['Winner'] == 'Blue']\n",
    "red_win = df[df['Winner'] == 'Red']\n",
    "\n",
    "blue_win_percent = len(blue_win) / len(df)\n",
    "red_win_percent = len(red_win) / len(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad97efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape[0]) # number of rows\n",
    "print(df.shape[1]) # number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc17039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.BlueStance.unique()\n",
    "#It has one spelling mistake\n",
    "df['BlueStance'].loc[df['BlueStance']=='Switch '] = 'Switch'\n",
    "#R_Stance doesn't have this error, so we're cool\n",
    "\n",
    "print(df['BlueStance'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca82444",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "red_win_percent_high_TDDif = len(red_win[red_win['AvgTDDif'] < -1]) / len(df[df['AvgTDDif'] < -1])\n",
    "print(\"Red Win Percentage Increase (w/ +1 more TDs): \" + str(red_win_percent_high_TDDif - red_win_percent))\n",
    "\n",
    "blue_win_percent_high_TDDif = len(blue_win[blue_win['AvgTDDif'] > 1]) / len(df[df['AvgTDDif'] > 1])\n",
    "print(\"Blue Win Percentage Increase (w/ +1 more TDs): \" + str(blue_win_percent_high_TDDif - blue_win_percent))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "red_win_percent_highTD_v_lowTD = len(red_win[(red_win['RedAvgTDLanded'] > 2.5) & (red_win['BlueAvgTDLanded'] < 1)]) / len(df[(df['RedAvgTDLanded'] > 2.5) & (df['BlueAvgTDLanded'] < 1)])\n",
    "print(\"Red Win Percentage Increase (w/ Red TD > 2.5, Blue TD < 1): \" + str(red_win_percent_highTD_v_lowTD - red_win_percent))\n",
    "\n",
    "blue_win_percent_highTD_v_lowTD = len(blue_win[(blue_win['BlueAvgTDLanded'] > 2.5) & (blue_win['RedAvgTDLanded'] < 1)]) / len(df[(df['BlueAvgTDLanded'] > 2.5) & (df['RedAvgTDLanded'] < 1)])\n",
    "print(\"Blue Win Percentage Increase (w/ Blue TD > 2.5, Red TD < 1): \" + str(blue_win_percent_highTD_v_lowTD - blue_win_percent))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "red_old_win_percent = len(df[(df['ROver35'] == 1) & (df['Winner'] == 'Red')]) / len(df[df['ROver35'] == 1])\n",
    "print(\"Decrease in Win Percentage When Red Fighter > 35 y.o.: \" + str(red_old_win_percent - red_win_percent))\n",
    "blue_old_win_percent = len(df[(df['BOver35'] == 1) & (df['Winner'] == 'Blue')]) / len(df[df['BOver35'] == 1])\n",
    "print(\"Decrease in Win Percentage When Blue Fighter > 35 y.o.: \" + str(blue_old_win_percent - blue_win_percent))\n",
    "\n",
    "\n",
    "df.to_csv('df.csv')\n",
    "\n",
    "print(df['FavoriteWins'].sum() / len(df))\n",
    "print(\"Blue Corner Win Percentage: \" + str(blue_win_percent))\n",
    "print(\"Red Corner Win Percentage: \" + str(red_win_percent))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].hist(red_win['RedOdds'], alpha=0.7, color='red')\n",
    "ax[0].hist(red_win['BlueOdds'], alpha=0.7, color='blue')\n",
    "ax[0].set_title('Odds for Red Wins')\n",
    "ax[1].hist(blue_win['RedOdds'], alpha=0.7, color='red')\n",
    "ax[1].hist(blue_win['BlueOdds'], alpha=0.7, color='blue')\n",
    "ax[1].set_title('Odds for Blue Wins')\n",
    "plt.show()\n",
    "\n",
    "sns.scatterplot(x='WinDif', y='LossDif', data=df, hue='RedAvgSigStrLanded')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf84bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calc missing proportions\n",
    "missing_proportions = df.isnull().sum() / len(df)\n",
    "print(\"Proportion of missing values per column:\")\n",
    "print(missing_proportions)\n",
    "\n",
    "#calc missing percentages\n",
    "missing_percentages = df.isnull().sum() * 100 / len(df)\n",
    "print(\"\\nPercentage of missing values per column:\")\n",
    "print(missing_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a4fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count columns with any missing data\n",
    "columns_with_missing_data = df.isna().any()\n",
    "number_of_columns_with_missing_data = columns_with_missing_data.sum()\n",
    "number_of_columns_with_missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334209cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape[0]) # number of rows\n",
    "print(df.shape[1]) # number of rows\n",
    "#% of missing values per column \n",
    "missing_percentage = df.isnull().sum() * 100 / len(df)\n",
    "print(missing_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155f0806",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Winner']\n",
    "X = df.drop(columns=['Winner'])\n",
    "\n",
    "# Encode target variable labels as integers\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3d7c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(df['BlueReach'], kde=True, color='purple')\n",
    "plt.title('Distribution of Blue Fighter Reach')\n",
    "plt.xlabel('Reach (cm)')\n",
    "plt.ylabel('Number of Fighters')\n",
    "plt.xlim(50, 90)  # limit x-axis range to be reasonable bc who has a 20in reach? \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aebbb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(df['ReachDif'], kde=True, color='purple')\n",
    "plt.title('Distribution of Reach')\n",
    "plt.xlabel('Reach (cm)')\n",
    "plt.ylabel('Number of Fighters')\n",
    "plt.xlim(0, 20)  # limit x-axis range to be reasonable bc who has a 20in reach? \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b7d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['StanceCombo'] = df['RedStance'].astype(str) + ' vs ' + df['BlueStance'].astype(str)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(data=df, x='RWinPct', y='BWinPct', hue='StanceCombo', palette='Set2', alpha=0.7)\n",
    "plt.title('Red Win % vs Blue Win % by Stance Matchup')\n",
    "plt.xlabel('Red Win Percentage')\n",
    "plt.ylabel('Blue Win Percentage')\n",
    "plt.legend(title='Stance Matchup', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984137f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8761d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=df,\n",
    "    x='avg_sig_str_pct_diff',\n",
    "    y='avg_TD_pct_diff',    \n",
    "    hue='TitleBout',\n",
    "    palette='Dark2',\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title('Sig stikes vs Sig takedowns by bout-type')\n",
    "plt.xlabel('sig strikes difference (%)')\n",
    "plt.ylabel('sig takedown difference (%)')\n",
    "plt.legend(title='TitleBout_binary', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ababa9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of all fighter names from the red and blue corner\n",
    "all_fighters_df = pd.concat([df.RedFighter, df.BlueFighter], ignore_index = True) \n",
    "\n",
    "# how many unique names there are\n",
    "all_fighters_nbr = all_fighters_df.nunique() \n",
    "print(all_fighters_nbr)\n",
    "\n",
    "# a list of all fighter names from the red and blue corner\n",
    "all_fighters_df = pd.concat([df.RedFighter, df.BlueFighter], ignore_index = True) \n",
    "\n",
    "#check how many unique names there are\n",
    "all_fighters_nbr = all_fighters_df.nunique() \n",
    "print(all_fighters_nbr)\n",
    "\n",
    "df['WeightClass'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3894842",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove possible leading and trailing spaces in all 'object' columns\n",
    "ufc_obj = df.select_dtypes(['object'])\n",
    "df[ufc_obj.columns] = ufc_obj.apply(lambda x: x.str.strip()) # Apply over every column function lambda\n",
    "#categorical \n",
    "print(df['Finish'].value_counts())\n",
    "#filtering\n",
    "kos_by_round = df[['Finish', 'FinishRound']].query('Finish == \"KO/TKO\"') # Create new filtered dataframe\n",
    "kos_by_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b8a530",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysing \"_odds\" variables\n",
    "sns.scatterplot(x=\"BlueOdds\", y=\"RedOdds\", hue=\"Winner\", data = df)\n",
    "df[\"Winner\"].loc[df[\"BlueOdds\"]>1].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2047efae",
   "metadata": {},
   "outputs": [],
   "source": [
    "KOs = sns.countplot(x = kos_by_round['FinishRound']);\n",
    "plt.title('Total number of KOs from dataset\\nby round', pad = 50, weight = 'bold') # \\n for line break. S: https://www.python-graph-gallery.com/190-custom-matplotlib-title\n",
    "plt.xlabel('Round number', labelpad = 20) # Label weight set to bold in general settings\n",
    "plt.ylabel('KOs', labelpad = 20)\n",
    "sns.despine() # Remove top and right border\n",
    "plt.ylim([0,1100])\n",
    "plt.bar_label(KOs.containers[0], weight = 'bold') # Add number labels on top of bars\n",
    "plt.text(x = 3, y = 800, s = 'Most KOs happen in the first round', fontdict = {'size' : 20, 'weight' : 'bold', 'color': 'orange'}, bbox=dict(facecolor='none', edgecolor='black', boxstyle='round,pad=1'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ecc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the age gap\n",
    "df['AgeGap'] = abs(df['RedAge'] - df['BlueAge'])\n",
    "df[['AgeGap', 'RedAge', 'BlueAge']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a89c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate the spread as positive number\n",
    "def spread_calculation(RedOdds, BlueOdds):\n",
    "    if RedOdds > 0 and BlueOdds > 0: \n",
    "        return max(RedOdds, BlueOdds) - min(RedOdds, BlueOdds)\n",
    "    else: \n",
    "        return abs(RedOdds - BlueOdds)\n",
    "\n",
    "# Create column that holds the spread\n",
    "df['Spread'] = df.apply(lambda row: spread_calculation(row['RedOdds'], row['BlueOdds']), axis=1)\n",
    "\n",
    "# Check results\n",
    "df[['RedOdds', 'BlueOdds','Spread']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d92dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.stripplot(data = df, x = 'AgeGap', y = 'Spread').set(title = 'Odds spread by age gap')\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1611de66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a frequency table \n",
    "win_counts = df['Winner'].value_counts()\n",
    "# Display the frequency table\n",
    "print(\"Frequency Table for 'Winner' column:\")\n",
    "print(win_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d28576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get proportions instead of counts\n",
    "check_win = df['Winner'].value_counts(normalize=True)\n",
    "print(\"\\nProportion Table for 'Winner' column:\")\n",
    "print(check_win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db10da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in WeightClass column:\", df['WeightClass'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d58776",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes) \n",
    "# How many rows and columns do we have in df_merge?\n",
    "print(df.shape[0]) # number of rows\n",
    "print(df.shape[1]) # number of rows\n",
    "\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f1ee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Count of Winners per Matchup type\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='RGrapplerVBStriker', hue='Winner', palette='Set2')\n",
    "plt.title('Match Outcome by RGrapplerVBStriker')\n",
    "plt.xlabel('RGrapplerVBStriker)')\n",
    "plt.ylabel('Number of Wins')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b57389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new combined column for Red vs Blue matchup\n",
    "#df['Matchup'] = df['RGrapplerVBStriker'].astype(str)\n",
    "\n",
    "# Plot: Count of Winners per Matchup type\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='RedIsStriker', hue='Winner', palette='Set2')\n",
    "plt.title('Match Outcome by RedIsStriker')\n",
    "plt.xlabel('RedIsStriker)')\n",
    "plt.ylabel('Number of Wins')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befed62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Count of Winners per Matchup type\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='BlueIsStriker', hue='Winner', palette='Set2')\n",
    "plt.title('Match Outcome by BlueIsStriker')\n",
    "plt.xlabel('BlueIsStriker)')\n",
    "plt.ylabel('Number of Wins')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638f5f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Convert TitleBout to string \"True\"/\"False\" for encoding\n",
    "if 'TitleBout' in df.columns:\n",
    "    df['TitleBout'] = df['TitleBout'].astype(str)\n",
    "\n",
    "\n",
    "# # 1. Map numeric codes to string class names (works on int, float, or string numbers)\n",
    "# weightclass_mapping = {\n",
    "#     '1': 'Heavyweight',\n",
    "#     '2': 'Light Heavyweight',\n",
    "#     '3': 'Middleweight',\n",
    "#     '4': 'Welterweight',\n",
    "#     '5': 'Lightweight',\n",
    "#     '6': 'Featherweight',\n",
    "#     '7': 'Bantamweight',\n",
    "#     '8': 'Flyweight',\n",
    "#     '9': 'Strawweight',\n",
    "#     '10': 'Catch Weight'\n",
    "# }\n",
    "\n",
    "# # Also map integer and float numeric values:\n",
    "# df['WeightClass'] = df['WeightClass'].map(lambda x: weightclass_mapping.get(str(x), x))\n",
    "\n",
    "# 2. Normalize string class names (strip, title case)\n",
    "# df['WeightClass'] = df['WeightClass'].astype(str).str.strip().str.title()\n",
    "\n",
    "# # 3. Fix known variants\n",
    "# weight_class_map = {'Catchweight': 'Catch Weight'}\n",
    "# df['WeightClass'] = df['WeightClass'].replace(weight_class_map)\n",
    "\n",
    "# # 4. Replace 'Nan' strings with np.nan (if any)\n",
    "# df['WeightClass'] = df['WeightClass'].replace(['Nan', 'nan'], np.nan)\n",
    "\n",
    "# # 5. Check unmapped values\n",
    "# allowed_classes = [\n",
    "# 'Flyweight' 'Welterweight' 'Heavyweight' 'Featherweight'\n",
    "#  'Light Heavyweight' 'Catch Weight' 'Lightweight' 'Bantamweight'\n",
    "#  \"Women's Strawweight\" \"Women's Flyweight\" 'Middleweight'\n",
    "#  \"Women's Bantamweight\" \"Women's Featherweight\"\n",
    "# ]\n",
    "\n",
    "# unmapped = set(df['WeightClass'].unique()) - set(allowed_classes)\n",
    "# if unmapped:\n",
    "#     print(f\"Warning: unmapped WeightClass values {unmapped}\")\n",
    "\n",
    "\n",
    "\n",
    "categorical_nominal = [\n",
    "    'Country', 'BlueStance', 'RedStance', 'BetterRank',\n",
    "    'Finish', 'FinishDetails', 'FinishRoundTime',\n",
    "    'Favorite', 'StanceCombo', 'TitleBout', \"WeightClass\"\n",
    "]\n",
    "\n",
    "# categorical_ordered = ['WeightClass']  # Now treated as ordinal numeric\n",
    "# ordinal_cats = [sorted(weightclass_labels.keys())]\n",
    "\n",
    "\n",
    "# ordinal_cats = [[\n",
    "#     'Heavyweight', 'Light Heavyweight', 'Middleweight', 'Welterweight', 'Lightweight',\n",
    "#     'Featherweight', 'Bantamweight', 'Flyweight', 'Strawweight', 'Catch Weight'\n",
    "# ]]\n",
    "\n",
    "numeric_features = [\n",
    "    'RedOdds', 'BlueOdds', 'RedExpectedValue', 'BlueExpectedValue',\n",
    "    'NumberOfRounds', 'BlueCurrentLoseStreak', 'BlueCurrentWinStreak', 'BlueDraws',\n",
    "    'BlueAvgSigStrLanded', 'BlueAvgSigStrPct', 'BlueAvgSubAtt', 'BlueAvgTDLanded', 'BlueAvgTDPct',\n",
    "    'BlueLongestWinStreak', 'BlueLosses', 'BlueTotalRoundsFought', 'BlueTotalTitleBouts',\n",
    "    'BlueWinsByDecisionMajority', 'BlueWinsByDecisionSplit', 'BlueWinsByDecisionUnanimous',\n",
    "    'BlueWinsByKO', 'BlueWinsBySubmission', 'BlueWinsByTKODoctorStoppage', 'BlueWins',\n",
    "    'BlueWeightLbs', 'BlueAge',\n",
    "\n",
    "    'RedCurrentLoseStreak', 'RedCurrentWinStreak', 'RedDraws',\n",
    "    'RedAvgSigStrLanded', 'RedAvgSigStrPct', 'RedAvgSubAtt', 'RedAvgTDLanded', 'RedAvgTDPct',\n",
    "    'RedLongestWinStreak', 'RedLosses', 'RedTotalRoundsFought', 'RedTotalTitleBouts',\n",
    "    'RedWinsByDecisionMajority', 'RedWinsByDecisionSplit', 'RedWinsByDecisionUnanimous',\n",
    "    'RedWinsByKO', 'RedWinsBySubmission', 'RedWinsByTKODoctorStoppage', 'RedWins',\n",
    "    'RedWeightLbs', 'RedAge',\n",
    "\n",
    "    'LoseStreakDif', 'WinStreakDif', 'LongestWinStreakDif', 'WinDif', 'LossDif', 'TotalRoundDif', 'TotalTitleBoutDif',\n",
    "    'KODif', 'SubDif', 'HeightDif', 'ReachDif', 'AgeDif', 'SigStrDif', 'AvgSubAttDif', 'AvgTDDif',\n",
    "    'EmptyArena',\n",
    "\n",
    "    'BMatchWCRank', 'RMatchWCRank',\n",
    "    'RWFlyweightRank', 'RWFeatherweightRank', 'RWStrawweightRank', 'RWBantamweightRank', 'RHeavyweightRank',\n",
    "    'RLightHeavyweightRank', 'RMiddleweightRank', 'RWelterweightRank', 'RLightweightRank', 'RFeatherweightRank',\n",
    "    'RBantamweightRank', 'RFlyweightRank', 'RPFPRank',\n",
    "    'BWFlyweightRank', 'BWFeatherweightRank', 'BWStrawweightRank', 'BWBantamweightRank', 'BHeavyweightRank',\n",
    "    'BLightHeavyweightRank', 'BMiddleweightRank', 'BWelterweightRank', 'BLightweightRank', 'BFeatherweightRank',\n",
    "    'BBantamweightRank', 'BFlyweightRank', 'BPFPRank',\n",
    "\n",
    "    'FinishRound', 'TotalFightTimeSecs',\n",
    "    'RedDecOdds', 'BlueDecOdds', 'RSubOdds', 'BSubOdds', 'RKOOdds', 'BKOOdds',\n",
    "\n",
    "    'BlueHeight', 'BlueReach', 'RedHeight', 'RedReach',\n",
    "\n",
    "    'ROver35', 'BOver35',\n",
    "\n",
    "    'RWinPct', 'BWinPct',\n",
    "    'RedStrikingRatio', 'BlueStrikingRatio',\n",
    "    'RedTotalFights', 'BlueTotalFights',\n",
    "    'RedIsGrappler', 'BlueIsGrappler', 'RedIsStriker', 'BlueIsStriker',\n",
    "    'RGrapplerVBStriker', 'BGrapplerVRStriker',\n",
    "    'RedStrikingEfficiency', 'BlueStrikingEfficiency',\n",
    "    'RedTDEfficiency', 'BlueTDEfficiency',\n",
    "    'RedEffectiveTD', 'BlueEffectiveTD',\n",
    "    'EffectiveTDDif',\n",
    "    'RedSize', 'BlueSize',\n",
    "    'FavoriteWins', 'draw_diff',\n",
    "    'avg_sig_str_pct_diff', 'avg_TD_pct_diff',\n",
    "    'win_by_Decision_Majority_diff', 'win_by_Decision_Split_diff', 'win_by_Decision_Unanimous_diff',\n",
    "    'win_by_TKO_Doctor_Stoppage_diff', 'odds_diff', 'ev_diff'\n",
    "]\n",
    "\n",
    "# --- 7. Replace infinite values with NaN in numeric features ---\n",
    "df[numeric_features] = df[numeric_features].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "# numeric_transformer = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='mean')),\n",
    "#     ('scaler', StandardScaler())\n",
    "# ])\n",
    "\n",
    "# categorical_nominal_transformer = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "# ])\n",
    "\n",
    "# categorical_ordered_transformer = Pipeline([\n",
    "#     ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "#     # ('ordinal', OrdinalEncoder(categories=ordinal_cats))\n",
    "# ])\n",
    "\n",
    "# collect all the encoders into one preprocessor\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numeric_transformer, numeric_features),\n",
    "#         ('cat', categorical_nominal_transformer, categorical_nominal)\n",
    "#     ])\n",
    "\n",
    "\n",
    "# prep = Pipeline(steps=[('preprocessor', preprocessor)]) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a44c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes) \n",
    "print(df.shape[0]) # number of rows\n",
    "print(df.shape[1]) # number of rows\n",
    "print(len(df))\n",
    "df.head()\n",
    "for column in df.columns:\n",
    "    unique_values = df[column].unique()\n",
    "    print(f\"Unique values in column '{column}': {unique_values}\")\n",
    "    print(f\"Number of unique values in column '{column}': {len(unique_values)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68bbd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f3134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(df.columns)\n",
    "print(df.columns.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecd3a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#  Convert 'Winner' to binary (Red=1, Blue=0), and drop others\n",
    "df = df[df['Winner'].isin(['Red', 'Blue'])]  # Keep only valid outcomes\n",
    "df['Winner'] = df['Winner'].map({'Red': 1, 'Blue': 0})  # Binary conversion\n",
    "\n",
    "# Check value counts for the 'Winner' column (including NaNs)\n",
    "print(df['Winner'].value_counts(dropna=False))\n",
    "\n",
    "# Basic descriptive statistics\n",
    "print(df.describe())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39279ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert to DataFrame for plotting\n",
    "corr_df = pd.DataFrame(list(filtered_corr_dict.items()), columns=['Feature', 'Correlation'])\n",
    "corr_df.sort_values(by='Correlation', ascending=False, inplace=True)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Correlation', y='Feature', data=corr_df, palette='viridis')\n",
    "\n",
    "plt.title(\"Top Features Correlated with Winner\", fontsize=14)\n",
    "plt.xlabel(\"Absolute Pearson Correlation with Winner\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e0cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#shiyu\n",
    "# To follow up on our previous discussion: \n",
    "# You can keep all 152 variables — that’s perfectly fine. \n",
    "# Just make sure to check for highly correlated features and remove or combine \n",
    "# any that show strong correlations (for example, |r| > 0.9), \n",
    "# so your model doesn’t suffer from redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f8ee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in random_states:\n",
    "\n",
    "#    - split the data\n",
    "#    - preprocess it\n",
    "#    - decide which hyperparameters you'll tune and what values you'll try\n",
    "#    - for combo in hyperparameters:\n",
    "#        - train your ML algo\n",
    "#        - calculate training scores\n",
    "#        - calculate validation scores\n",
    "#    - select best model based on the mean and std validation scores\n",
    "#    - predict the test set using the best model\n",
    "#    - return your test score (generalization error)\n",
    "#    - return the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6748ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot missing values heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(df.isnull(), cbar=False, cmap=\"viridis\")\n",
    "plt.title(\"Missing Data Heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Missing values per column (%)\n",
    "missing_percent = df.isnull().sum() * 100 / len(df)\n",
    "print(\"Missing percentages per column:\\n\", missing_percent)\n",
    "\n",
    "# Class balance\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='Winner', data=df)\n",
    "plt.title('Class Balance: Winner')\n",
    "plt.show()\n",
    "\n",
    "# Display proportions\n",
    "print(\"Class proportions:\")\n",
    "print(df['Winner'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf8d3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate categorical and numerical columns\n",
    "cat_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "num_cols.remove('Winner') if 'Winner' in num_cols else None\n",
    "\n",
    "print(\"Categorical columns:\", cat_cols)\n",
    "print(\"Numerical columns:\", num_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3f36e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DummyClassifier = baseline to compare a better model's performance later \n",
    "# base_model = DummyClassifier(random_state=42)\n",
    "# base_model.fit(X_train,y_train)\n",
    "# preds = base_model.predict(X_valid)\n",
    "# accuracy_score(y_valid, preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed43d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# ------------------------------------------------\n",
    "# columns with excessive missingness\n",
    "# ------------------------------------------------\n",
    "cols_missing = [\n",
    "    'BMatchWCRank', 'RMatchWCRank', 'RWFlyweightRank', 'RWFeatherweightRank',\n",
    "    'RWStrawweightRank', 'RWBantamweightRank', 'RHeavyweightRank',\n",
    "    'RLightHeavyweightRank', 'RMiddleweightRank', 'RWelterweightRank',\n",
    "    'RLightweightRank', 'RFeatherweightRank', 'RBantamweightRank',\n",
    "    'RFlyweightRank', 'RPFPRank', 'BWFlyweightRank', 'BWFeatherweightRank',\n",
    "    'BWStrawweightRank', 'BWBantamweightRank', 'BHeavyweightRank',\n",
    "    'BLightHeavyweightRank', 'BMiddleweightRank', 'BWelterweightRank',\n",
    "    'BLightweightRank', 'BFeatherweightRank', 'BBantamweightRank',\n",
    "    'BFlyweightRank', 'BPFPRank'\n",
    "]\n",
    "\n",
    "# ------------------------------------------------\n",
    "# columns to drop due to data leakage\n",
    "# ------------------------------------------------\n",
    "cols_leakage = [\n",
    "# Red fighter stats\n",
    "'RedAvgSigStrLanded', 'RedAvgSigStrPct', 'RedAvgSubAtt',\n",
    "'RedAvgTDLanded', 'RedAvgTDPct', 'RedLongestWinStreak',\n",
    "'RedTotalRoundsFought', 'RedTotalTitleBouts',\n",
    "'RedWinsByDecisionMajority', 'RedWinsByDecisionSplit', 'RedWinsByDecisionUnanimous',\n",
    "'RedWinsByKO', 'RedWinsBySubmission', 'RedWinsByTKODoctorStoppage',\n",
    "'RedWins', 'RedStrikingRatio', 'RedStrikingEfficiency', 'RedTDEfficiency',\n",
    "'RedEffectiveTD', 'RedExpectedValue', 'RedDecOdds', 'RSubOdds', 'RKOOdds',\n",
    "\n",
    "# Blue fighter stats\n",
    "'BlueAvgSigStrLanded', 'BlueAvgSigStrPct', 'BlueAvgSubAtt',\n",
    "'BlueAvgTDLanded', 'BlueAvgTDPct', 'BlueLongestWinStreak',\n",
    "'BlueTotalRoundsFought', 'BlueTotalTitleBouts',\n",
    "'BlueWinsByDecisionMajority', 'BlueWinsByDecisionSplit', 'BlueWinsByDecisionUnanimous',\n",
    "'BlueWinsByKO', 'BlueWinsBySubmission', 'BlueWinsByTKODoctorStoppage',\n",
    "'BlueWins', 'BlueStrikingRatio', 'BlueStrikingEfficiency', 'BlueTDEfficiency',\n",
    "'BlueEffectiveTD', 'BlueExpectedValue', 'BlueDecOdds', 'BSubOdds', 'BKOOdds',\n",
    "'Finish', 'FinishDetails', 'FinishRound', 'FinishRoundTime', 'TotalFightTimeSecs', 'Favorite', 'FavoriteWins', 'EmptyArena',\n",
    "\n",
    "# Derived features (includes current fight outcome) \n",
    "'SigStrDif', 'AvgSubAttDif', 'AvgTDDif', 'LoseStreakDif', 'WinStreakDif', 'LongestWinStreakDif', \n",
    "'WinDif', 'LossDif', 'TotalRoundDif', 'TotalTitleBoutDif', 'KODif', 'SubDif', \n",
    "'NumberOfRounds', 'EffectiveTDDif', 'draw_diff', 'avg_sig_str_pct_diff', 'avg_TD_pct_diff', \n",
    "'win_by_Decision_Majority_diff', 'win_by_Decision_Split_diff', 'win_by_Decision_Unanimous_diff', 'win_by_TKO_Doctor_Stoppage_dif' \n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "df_clean = df.drop(columns=cols_missing + cols_leakage, errors='ignore')\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# drop strongly corr ftrs  (|r| ≥ 0.50)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "num_df = df_clean.select_dtypes(include=['int64','float64'])\n",
    "corr_matrix = num_df.corr().abs()\n",
    "\n",
    "# Keep only upper triangle\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Strong correlation pairs\n",
    "strong_pairs = (\n",
    "    upper.stack()\n",
    "         .reset_index()\n",
    "         .rename(columns={\"level_0\": \"Feature_1\", \"level_1\": \"Feature_2\", 0: \"Correlation\"})\n",
    ")\n",
    "strong_pairs = strong_pairs[strong_pairs[\"Correlation\"] >= 0.50]\n",
    "\n",
    "print(\"\\nSTRONGLY CORRELATED PAIRS (|r| ≥ 0.50)\")\n",
    "print(strong_pairs.sort_values(\"Correlation\", ascending=False))\n",
    "\n",
    "# Choose lower variance feature from each pair\n",
    "to_drop_corr = set()\n",
    "for f1, f2, corr_val in strong_pairs.values:\n",
    "    var1 = num_df[f1].var()\n",
    "    var2 = num_df[f2].var()\n",
    "    drop_feature = f1 if var1 < var2 else f2\n",
    "    to_drop_corr.add(drop_feature)\n",
    "\n",
    "print(\"\\nDropping correlated features:\")\n",
    "print(to_drop_corr)\n",
    "\n",
    "# Drop from df_clean\n",
    "df_clean = df_clean.drop(columns=list(to_drop_corr), errors='ignore')\n",
    "\n",
    "# Rebuild num_df using cleaned df\n",
    "num_df = df_clean.select_dtypes(include=['int64','float64'])\n",
    "\n",
    "# ---------------------\n",
    "# Load features + target\n",
    "# ---------------------\n",
    "X = df_clean.drop(columns=['Winner'])\n",
    "y = df_clean['Winner']\n",
    "\n",
    "print(\"Raw feature columns:\")\n",
    "print(X.columns.tolist())\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nFeatures considered for modeling (Red-only):\")\n",
    "print(X.columns.tolist())\n",
    "\n",
    "# -----------------------------------------\n",
    "#                EDA\n",
    "# -----------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\" BASIC DATA OVERVIEW\")\n",
    "print(\"==============================\")\n",
    "print(df_clean.head())\n",
    "print(\"\\nShape:\", df_clean.shape)\n",
    "print(\"\\nData types:\\n\", df_clean.dtypes)\n",
    "print(\"\\nMissing values per column:\\n\", df_clean.isna().sum().sort_values(ascending=False))\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Target variable distribution\n",
    "# -----------------------------------------------------------\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=df_clean['Winner'])\n",
    "plt.title(\"Distribution of Winner\")\n",
    "plt.xlabel(\"Winner (0 = Blue, 1 = Red)\")\n",
    "plt.show()\n",
    "\n",
    "num_df = X.select_dtypes(include=['int64','float64'])\n",
    "cat_df = X.select_dtypes(include=['object', 'category'])\n",
    "\n",
    "print(\"\\nUpdated feature count after removing correlated vars:\")\n",
    "print(\"Numeric:\", len(num_df.columns))\n",
    "print(\"Categorical:\", len(cat_df.columns))\n",
    "\n",
    "# rebuild cleaned num_df\n",
    "num_df = df_clean.select_dtypes(include=['int64','float64'])\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.histplot(num_df.isna().sum(), bins=20)\n",
    "plt.title(\"Missing Values Distribution (Numeric Columns)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNumeric Summary Stats:\\n\")\n",
    "display(num_df.describe().T)\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Correlation Heatmap (top 20 strongest correlations)\n",
    "# -----------------------------------------------------------\n",
    "plt.figure(figsize=(14,12))\n",
    "corr = num_df.corr()\n",
    "\n",
    "# keep strongest correlations with target\n",
    "if 'Winner' in corr.columns:\n",
    "    target_corr = corr['Winner'].abs().sort_values(ascending=False)[1:21]\n",
    "    top_corr_cols = target_corr.index.tolist()\n",
    "else:\n",
    "    top_corr_cols = num_df.columns[:20]\n",
    "\n",
    "sns.heatmap(num_df[top_corr_cols].corr(), annot=False, cmap='viridis')\n",
    "plt.title(\"Correlation Heatmap (Top Correlated Numeric Features)\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Distribution plots for key variables\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "cols_to_plot = [\n",
    "    'ReachDif', 'HeightDif', 'Oddsdiff',\n",
    "    'ev_diff', 'AgeGap'\n",
    "]\n",
    "cols_to_plot = [c for c in cols_to_plot if c in df_clean.columns]\n",
    "\n",
    "for col in cols_to_plot:\n",
    "    plt.figure(figsize=(7,4))\n",
    "    sns.histplot(df_clean[col], kde=True)\n",
    "    plt.title(f\"Distribution: {col}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Boxplots of numeric variables by Winner\n",
    "# -----------------------------------------------------------\n",
    "for col in cols_to_plot:\n",
    "    plt.figure(figsize=(7,4))\n",
    "    sns.boxplot(x=df_clean[\"Winner\"], y=df_clean[col])\n",
    "    plt.title(f\"{col} by Winner\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Categorical feature frequencies\n",
    "# -----------------------------------------------------------\n",
    "cat_df = df_clean.select_dtypes(include=['object','category'])\n",
    "\n",
    "for col in cat_df.columns:\n",
    "    plt.figure(figsize=(9,4))\n",
    "    sns.countplot(y=df_clean[col], order=df_clean[col].value_counts().index)\n",
    "    plt.title(f\"Value Counts: {col}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n-----------------\")\n",
    "print(\" EDA COMPLETE\")\n",
    "print(\"-----------------\")\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Recompute num/cat columns after feature filtering\n",
    "# ---------------------------------------------------\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(\"\\nNumeric columns:\", len(num_cols))\n",
    "print(\"Categorical columns:\", len(cat_cols))\n",
    "\n",
    "# ---------------------\n",
    "# Encode target\n",
    "# ---------------------\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "# ===========================================================\n",
    "#         train/test split (matchup-safe so no leakage)\n",
    "# ===========================================================\n",
    "\n",
    "print(\"\\n==============================\")\n",
    "print(\" Creating matchup-safe split\")\n",
    "print(\"==============================\")\n",
    "\n",
    "\n",
    "# create MatchupID safely convert values to string\n",
    "df_clean[\"MatchupID\"] = df_clean.apply(\n",
    "    lambda row: \"_\".join(\n",
    "        sorted([\n",
    "            str(row.get(\"RedFighter\", \"\")),\n",
    "            str(row.get(\"BlueFighter\", \"\"))\n",
    "        ])\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "# rebuild X and y including MatchupID\n",
    "X = df_clean.drop(columns=['Winner'])\n",
    "y = df_clean['Winner']\n",
    "y_enc = le.transform(y)    \n",
    "\n",
    "print(\"\\n=== SANITY CHECK: Features & Target ===\")\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"X columns:\", X.columns.tolist())\n",
    "print(\"y unique values:\", np.unique(y))\n",
    "\n",
    "\n",
    "# group-based split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y_enc, groups=df_clean[\"MatchupID\"]))\n",
    "\n",
    "X_train = X.iloc[train_idx].reset_index(drop=True)\n",
    "X_test  = X.iloc[test_idx].reset_index(drop=True)\n",
    "y_train = y_enc[train_idx]\n",
    "y_test  = y_enc[test_idx]\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\",  X_test.shape)\n",
    "print(\"Unique matchups in train:\", df_clean.iloc[train_idx][\"MatchupID\"].nunique())\n",
    "print(\"Unique matchups in test:\",  df_clean.iloc[test_idx][\"MatchupID\"].nunique())\n",
    "\n",
    "# remove identity columns not allowed in modeling\n",
    "drop_cols = [\"RedFighter\", \"BlueFighter\", \"MatchupID\"]\n",
    "X_train = X_train.drop(columns=[c for c in drop_cols if c in X_train.columns])\n",
    "X_test  = X_test.drop(columns=[c for c in drop_cols if c in X_test.columns])\n",
    "\n",
    "groups_train = df_clean.iloc[train_idx][\"MatchupID\"].values\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#   RECOMPUTE NUMERIC COLUMNS AFTER FINAL FEATURE SELECTION\n",
    "# --------------------------------------------------------------------\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "\n",
    "print(\"\\nUpdated numeric columns:\", len(num_cols))\n",
    "print(\"Updated categorical columns:\", len(cat_cols))\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#  MICEFOREST IMPUTATION (numeric columns only)\n",
    "# --------------------------------------------------------------------\n",
    "import miceforest as mf\n",
    "\n",
    "print(\"\\nRunning MICE-Forest Imputation on numeric columns...\")\n",
    "\n",
    "# Reset index and ensure numeric columns only\n",
    "X_train_num = X_train[num_cols].reset_index(drop=True)\n",
    "X_test_num = X_test[num_cols].reset_index(drop=True)\n",
    "\n",
    "# Ensure column names are strings (required by miceforest)\n",
    "X_train_num.columns = X_train_num.columns.astype(str)\n",
    "X_test_num.columns = X_test_num.columns.astype(str)\n",
    "\n",
    "# Combine train + test numeric columns\n",
    "X_full_num = pd.concat([X_train_num, X_test_num], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Build kernel\n",
    "kernel = mf.ImputationKernel(\n",
    "    data=X_full_num,\n",
    "    num_datasets=1,\n",
    "    variable_schema=None,\n",
    "    random_state=42,\n",
    "    save_all_iterations_data=False\n",
    ")\n",
    "\n",
    "# Run MICE\n",
    "kernel.mice(\n",
    "    iterations=3,\n",
    "    n_estimators=50\n",
    ")\n",
    "\n",
    "#pipx install cookiecutter-data-science\n",
    "\n",
    "\n",
    "# Extract completed numeric data\n",
    "X_full_num_completed = kernel.complete_data(dataset=0)\n",
    "\n",
    "# Split back into train/test\n",
    "completed_train = X_full_num_completed.iloc[:len(X_train)].copy()\n",
    "completed_test  = X_full_num_completed.iloc[len(X_train):].copy()\n",
    "\n",
    "# Align columns with original order to avoid the permutation issues -> did not work rats \n",
    "completed_train = completed_train[num_cols]\n",
    "completed_test  = completed_test[num_cols]\n",
    "\n",
    "X_train[num_cols] = completed_train.values\n",
    "X_test[num_cols] = completed_test.values\n",
    "\n",
    "\n",
    "print(\"MICE Imputation Completed.\\n\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Preprocessing pipeline\n",
    "# ----------------------------\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer  # still needed for categorical\n",
    "\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())  # numeric already imputed\n",
    "])\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afee5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, GridSearchCV, KFold\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, roc_auc_score, classification_report\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#  Machine Learning Pipelines\n",
    "# --------------------------------------------------------------------\n",
    "pipelines = {\n",
    "    'LogisticRegression': Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(max_iter=1000))\n",
    "    ]),\n",
    "\n",
    "    'RandomForest': Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ]),\n",
    "\n",
    "    'SVM': Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', SVC(probability=True))\n",
    "    ]),\n",
    "\n",
    "    'XGBoost': Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', XGBClassifier(eval_metric='logloss'))\n",
    "    ])\n",
    "}\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#  Hyperparameter Grids\n",
    "# --------------------------------------------------------------------\n",
    "param_grids = {\n",
    "    'LogisticRegression': {'classifier__C': [0.01, 0.1, 1, 10]},\n",
    "    'RandomForest': {\n",
    "        'classifier__n_estimators': [100, 300],\n",
    "        'classifier__max_depth': [None, 10, 20],\n",
    "        'classifier__min_samples_split': [2, 5]\n",
    "    },\n",
    "\n",
    "    'SVM': {\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "        'classifier__kernel': ['rbf', 'linear']\n",
    "    },\n",
    "\n",
    "    'XGBoost': {\n",
    "        'classifier__n_estimators': [100, 300],\n",
    "        'classifier__max_depth': [3, 6],\n",
    "        'classifier__learning_rate': [0.01, 0.1]\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nr_states = 3\n",
    "kf_splits = 5\n",
    "results_summary = {}\n",
    "\n",
    "drop_cols = ['RedFighter', 'BlueFighter', 'MatchupID']\n",
    "\n",
    "num_cols = [c for c in X_train.select_dtypes(include=['int64', 'float64']).columns if c not in drop_cols]\n",
    "cat_cols = [c for c in X_train.select_dtypes(include=['object', 'category']).columns if c not in drop_cols]\n",
    "\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nr_states = 3\n",
    "kf_splits = 5\n",
    "results_summary = {}\n",
    "\n",
    "drop_cols = ['RedFighter', 'BlueFighter', 'MatchupID']\n",
    "\n",
    "num_cols = [c for c in X_train.select_dtypes(include=['int64', 'float64']).columns if c not in drop_cols]\n",
    "cat_cols = [c for c in X_train.select_dtypes(include=['object', 'category']).columns if c not in drop_cols]\n",
    "\n",
    "\n",
    "for name, base_pipeline in pipelines.items(): # rename base_model to base_pipeline for clarity\n",
    "    print(f\"\\n======================\")\n",
    "    print(f\"Training {name}\")\n",
    "    print(f\"======================\")\n",
    "\n",
    "    test_scores = []\n",
    "    best_param_list = []\n",
    "    final_models = []\n",
    "\n",
    "    for i in range(nr_states):\n",
    "        current_state = i + 1\n",
    "        print(f\"Random State {current_state}\")\n",
    "\n",
    "        pipeline = clone(base_pipeline)\n",
    "\n",
    "        if name in [\"RandomForest\", \"XGBoost\", \"LogisticRegression\"]: \n",
    "            pipeline.named_steps['classifier'].set_params(random_state=current_state)\n",
    "            \n",
    "        cv = StratifiedKFold(n_splits=kf_splits, shuffle=True, random_state=current_state)\n",
    "\n",
    "        grid = GridSearchCV( \n",
    "            pipeline, # Use the cloned pipeline\n",
    "            param_grid=param_grids[name], \n",
    "            scoring='accuracy', \n",
    "            cv=cv, \n",
    "            n_jobs=-1 \n",
    "        )\n",
    "        #shiyu \n",
    "        #ERROR FIX:  The error occurred here because the inner preprocessor definition was incomplete/redundant.\n",
    "        #Permutation importance → Your X_test and y_test likely don’t have the exact same number of rows after preprocessing.\n",
    "    #SHAP global importance → One or more columns in X_test is still not strictly 1-dimensional (e.g., an object/list column).\n",
    "    #Both are pretty straightforward to debug — you can print the shapes of X_test, y_test, and model.predict(X_test) \n",
    "    # PS10 is also a good reference.\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        best_model = grid.best_estimator_\n",
    "        best_params = grid.best_params_\n",
    "        best_param_list.append(best_params)\n",
    "        final_models.append(best_model)\n",
    "\n",
    "        y_test_pred = best_model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_test_pred)\n",
    "        test_scores.append(acc)\n",
    "\n",
    "        print(f\"Best Params: {best_params}\")\n",
    "        print(f\"Validation Accuracy (CV): {grid.best_score_:.4f}\")\n",
    "        print(f\"Test Accuracy: {acc:.4f}\\n\")\n",
    "\n",
    "    avg_test_score = np.mean(test_scores)\n",
    "    print(f\"Average Test Accuracy for {name}: {avg_test_score:.4f}\")\n",
    "\n",
    "    results_summary[name] = {\n",
    "        'test_scores': test_scores,\n",
    "        'avg_test_score': avg_test_score,\n",
    "        'best_params': best_param_list,\n",
    "        'models': final_models\n",
    "    }\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # Save the best model to disk\n",
    "    # --------------------------------------------------------------------\n",
    "    import joblib\n",
    "\n",
    "    best_model_overall = final_models[0]   # best model for this algorithm\n",
    "    save_path = f\"best_model_{name}.pkl\"\n",
    "    joblib.dump(best_model_overall, save_path)\n",
    "\n",
    "    print(f\"Saved best {name} model → {save_path}\")\n",
    "\n",
    "\n",
    "# results table based on rubric \n",
    "table_rows = []\n",
    "\n",
    "for model_name, info in results_summary.items():\n",
    "\n",
    "    # Best val score is the highest CV accuracy across random states\n",
    "    best_val_score = max([\n",
    "        max_state for max_state in info['test_scores']\n",
    "    ])\n",
    "\n",
    "    # Best parameter set (use the one from random state 0)\n",
    "    best_params = info[\"best_params\"][0]\n",
    "\n",
    "    # Mean test score across random states\n",
    "    mean_test_score = np.mean(info[\"test_scores\"])\n",
    "\n",
    "    # SD\n",
    "    std_test_score = np.std(info[\"test_scores\"])\n",
    "\n",
    "    # Best model for test score (random state 0)\n",
    "    best_model = info[\"models\"][0]\n",
    "\n",
    "    table_rows.append({\n",
    "        \"Algorithms\": model_name,\n",
    "        \"Parameters\": best_params,\n",
    "        \"Mean test score\": round(mean_test_score, 4),\n",
    "        \"Standard deviation\": round(std_test_score, 4),\n",
    "        \"Best model validation score\": round(best_val_score, 4),\n",
    "        \"Test score\": round(info[\"avg_test_score\"], 4)\n",
    "    })\n",
    "\n",
    "results_table = pd.DataFrame(table_rows)\n",
    "\n",
    "print(\"\\n\\n===============================\")\n",
    "print(\"       FINAL RESULTS TABLE\")\n",
    "print(\"===============================\\n\")\n",
    "print(results_table)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Plot: Mean Test Accuracy by Model Algorithm\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(\n",
    "    data=results_table,\n",
    "    x='Algorithms',\n",
    "    y='Mean test score',\n",
    "    ci=None\n",
    ")\n",
    "\n",
    "# Add error bars manually using standard deviation\n",
    "for i, row in results_table.iterrows():\n",
    "    plt.errorbar(\n",
    "        i,\n",
    "        row['Mean test score'],\n",
    "        yerr=row['Standard deviation'],\n",
    "        fmt='none',\n",
    "        capsize=5,\n",
    "        color='black'\n",
    "    )\n",
    "\n",
    "plt.title(\"Mean Test Accuracy by Model (with Standard Deviation)\")\n",
    "plt.xlabel(\"Model Algorithm\")\n",
    "plt.ylabel(\"Mean Test Accuracy\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# more results \n",
    "import numpy as np\n",
    "import shap\n",
    "from sklearn.metrics import accuracy_score, fbeta_score, f1_score, classification_report\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# fit a copy of the preprocessor on training data so I can extract names and transformed arrays\n",
    "fitted_preprocessor = preprocessor.fit(X_train)\n",
    "\n",
    "try:\n",
    "    feature_names = fitted_preprocessor.get_feature_names_out()\n",
    "except Exception:\n",
    "    feature_names = X_train.columns.tolist()\n",
    "\n",
    "X_train_trans = fitted_preprocessor.transform(X_train)\n",
    "X_test_trans  = fitted_preprocessor.transform(X_test)\n",
    "\n",
    "if hasattr(X_train_trans, \"toarray\"):\n",
    "    X_train_dense = X_train_trans.toarray()\n",
    "    X_test_dense  = X_test_trans.toarray()\n",
    "else:\n",
    "    X_train_dense = np.asarray(X_train_trans)\n",
    "    X_test_dense  = np.asarray(X_test_trans)\n",
    "\n",
    "# ---------------------------\n",
    "# baselines: majority-class predictions\n",
    "# ---------------------------\n",
    "# find majority label in training set\n",
    "(unique, counts) = np.unique(y_train, return_counts=True)\n",
    "majority_label = unique[np.argmax(counts)]\n",
    "baseline_preds = np.full_like(y_test, fill_value=majority_label)\n",
    "\n",
    "baseline_acc = accuracy_score(y_test, baseline_preds)\n",
    "# macro F-beta (bc works multi-class or binary)\n",
    "baseline_f05 = fbeta_score(y_test, baseline_preds, beta=0.5, average='macro', zero_division=0)\n",
    "baseline_f1  = f1_score(y_test, baseline_preds, average='macro', zero_division=0)\n",
    "baseline_f2  = fbeta_score(y_test, baseline_preds, beta=2, average='macro', zero_division=0)\n",
    "\n",
    "print(\"\\n=== Baseline (majority class) ===\")\n",
    "print(f\"Majority label: {majority_label}\")\n",
    "print(f\"Baseline accuracy: {baseline_acc:.4f}\")\n",
    "print(f\"Baseline F0.5 (macro): {baseline_f05:.4f}\")\n",
    "print(f\"Baseline F1 (macro): {baseline_f1:.4f}\")\n",
    "print(f\"Baseline F2 (macro): {baseline_f2:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\n-----------DEBUGGING SHAP & PERMUTATION INPUTS -------------\")\n",
    "\n",
    "print(\"Raw X_train shape:\", X_train.shape)\n",
    "print(\"Raw X_test shape:\", X_test.shape)\n",
    "print(\"Raw y_train shape:\", y_train.shape)\n",
    "print(\"Raw y_test shape:\", y_test.shape)\n",
    "\n",
    "# Checking for accidental non-1D columns inside X_train/X_test\n",
    "bad_columns = []\n",
    "for col in X_train.columns:\n",
    "    if isinstance(X_train[col].iloc[0], (list, dict, tuple)):\n",
    "        bad_columns.append(col)\n",
    "\n",
    "#lil emoji for my eyesight \n",
    "if bad_columns:\n",
    "    print(\"⚠️ WARNING: These columns are not 1-dimensional:\", bad_columns)\n",
    "    print(\"Attempting to flatten or drop them...\")\n",
    "    X_train = X_train.drop(columns=bad_columns)\n",
    "    X_test = X_test.drop(columns=bad_columns)\n",
    "\n",
    "# Force categorical columns to strings to prevent SHAP crash\n",
    "for col in cat_cols:\n",
    "    X_train[col] = X_train[col].astype(str)\n",
    "    X_test[col] = X_test[col].astype(str)\n",
    "\n",
    "# Validate preprocessing outputs  \n",
    "fitted_preprocessor = preprocessor.fit(X_train)\n",
    "\n",
    "X_train_trans = fitted_preprocessor.transform(X_train)\n",
    "X_test_trans  = fitted_preprocessor.transform(X_test)\n",
    "\n",
    "print(\"\\n=== SANITY CHECK: Preprocessed Data ===\")\n",
    "print(\"X_train_trans shape:\", X_train_trans.shape)\n",
    "print(\"X_test_trans shape:\", X_test_trans.shape)\n",
    "\n",
    "# Ensure no NaNs for numeric features\n",
    "print(\"Any NaNs in transformed X_train:\", np.isnan(X_train_trans).any())\n",
    "print(\"Any NaNs in transformed X_test:\", np.isnan(X_test_trans).any())\n",
    "\n",
    "\n",
    "# Convert sparse -> dense for SHAP + permutation importance\n",
    "if hasattr(X_train_trans, \"toarray\"):\n",
    "    X_train_dense = X_train_trans.toarray()\n",
    "    X_test_dense  = X_test_trans.toarray()\n",
    "else:\n",
    "    X_train_dense = np.asarray(X_train_trans)\n",
    "    X_test_dense  = np.asarray(X_test_trans)\n",
    "\n",
    "print(\"Transformed X_train:\", X_train_dense.shape)\n",
    "print(\"Transformed X_test:\", X_test_dense.shape)\n",
    "\n",
    "# Check mismatch between predictions and y_test -> still breaks permutation importance\n",
    "for model_name, info in results_summary.items():\n",
    "    model_check = info[\"models\"][0]\n",
    "    \n",
    "    try:\n",
    "        preds_check = model_check.predict(X_test)\n",
    "        print(f\"{model_name} predict(X_test) shape:\", preds_check.shape)\n",
    "        if preds_check.shape[0] != y_test.shape[0]:\n",
    "            print(\"❌ ERROR: Prediction length does NOT match y_test length - rats!\")\n",
    "        else:\n",
    "            print(\"✅ Yay! prediction length OK.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Boo - Prediction test failed for {model_name}: {e}\")\n",
    "\n",
    "print(\"------- Finished Debugging Block --------\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# compare models to baseline; compute permutation + impurity + SHAP importances \n",
    "# --------------------------------------------------------------------\n",
    "summary_rows = []\n",
    "global_importances_all = []  \n",
    "\n",
    "for model_name, info in results_summary.items():\n",
    "    print(f\"\\n\\n---------ANALYZING {model_name} ---------\")\n",
    "\n",
    "    # best pipeline/model: using model at index 0 ( model stored per random state)\n",
    "    pipeline_best = info['models'][0]\n",
    "    classifier = pipeline_best.named_steps['classifier']\n",
    "\n",
    "    # pred + basic metrics on test set\n",
    "    y_test_pred = pipeline_best.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_test_pred)\n",
    "    f05 = fbeta_score(y_test, y_test_pred, beta=0.5, average='macro', zero_division=0)\n",
    "    f1  = f1_score(y_test, y_test_pred, average='macro', zero_division=0)\n",
    "    f2  = fbeta_score(y_test, y_test_pred, beta=2, average='macro', zero_division=0)\n",
    "\n",
    "    # Mean and SD of test scores across random states (already stored as avg_test_score and test_scores)\n",
    "    mean_test = info.get('avg_test_score', np.mean(info['test_scores']))\n",
    "    std_test  = np.std(info['test_scores']) if 'test_scores' in info else np.nan\n",
    "\n",
    "    # of std dev above baseline accuracy (if std != 0)\n",
    "    if std_test and std_test > 0:\n",
    "        z_score = (mean_test - baseline_acc) / std_test\n",
    "    else:\n",
    "        z_score = np.nan\n",
    "\n",
    "    print(f\"Mean CV/Test (avg across states): {mean_test:.4f}  (std across states: {std_test:.4f})\")\n",
    "    print(f\"Test accuracy (best model): {acc:.4f}\")\n",
    "    print(f\"F0.5/F1/F2 (test): {f05:.4f} / {f1:.4f} / {f2:.4f}\")\n",
    "    print(f\"Z-score above baseline accuracy: {z_score if not np.isnan(z_score) else 'N/A'}\")\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # impurity importance (tree-based only)\n",
    "    # --------------------------------------------------------------------\n",
    "    impurity_df = None\n",
    "    if hasattr(classifier, \"feature_importances_\"):\n",
    "        try:\n",
    "            imp = classifier.feature_importances_\n",
    "            impurity_df = pd.DataFrame({\n",
    "                \"feature\": feature_names,\n",
    "                \"impurity_importance\": imp\n",
    "            }).sort_values(\"impurity_importance\", ascending=False)\n",
    "            print(\"Impurity importance computed.\")\n",
    "        except Exception as e:\n",
    "            print(\"Impurity importance failed:\", e)\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # permutation importance (model-agnostic)\n",
    "    # --------------------------------------------------------------------\n",
    "    perm_df = None\n",
    "    try:\n",
    "        #this works so revert later if needed \n",
    "        # perm = permutation_importance(\n",
    "        #     pipeline_best, X_test, y_test,\n",
    "        #     n_repeats=20, random_state=42, n_jobs=-1, scoring='accuracy'\n",
    "        # )\n",
    "\n",
    "        perm = permutation_importance(\n",
    "                pipeline_best,      # full pipeline\n",
    "                X_test,             # raw test set (not preprocessed)\n",
    "                y_test,\n",
    "                n_repeats=20,\n",
    "                random_state=42,\n",
    "                n_jobs=-1,\n",
    "                scoring='accuracy'\n",
    "            )  \n",
    "\n",
    "        perm_df = pd.DataFrame({\n",
    "            \"feature\": feature_names,\n",
    "            \"perm_mean\": perm.importances_mean,\n",
    "            \"perm_std\": perm.importances_std\n",
    "        }).sort_values(\"perm_mean\", ascending=False)\n",
    "        print(\"Permutation importance computed.\")\n",
    "    except Exception as e:\n",
    "        print(\"Permutation importance failed:\", e)\n",
    "\n",
    "\n",
    "# fitted preprocessor from the best pipeline\n",
    "    fitted_preprocessor_step = pipeline_best.named_steps['preprocessor']\n",
    "\n",
    "    # fransform the raw data (X_train and X_test) \n",
    "    X_train_trans = pipeline_best.named_steps['preprocessor'].transform(X_train)\n",
    "    X_test_trans  = pipeline_best.named_steps['preprocessor'].transform(X_test)\n",
    "\n",
    "    # Convert sparse -> dense\n",
    "    X_train_dense = X_train_trans.toarray() if hasattr(X_train_trans, \"toarray\") else X_train_trans\n",
    "    X_test_dense  = X_test_trans.toarray() if hasattr(X_test_trans, \"toarray\") else X_test_trans\n",
    "\n",
    "\n",
    "    # convert to dense for SHAP/Permutation \n",
    "    if hasattr(X_train_trans, \"toarray\"):\n",
    "        X_train_dense = X_train_trans.toarray()\n",
    "    else:\n",
    "        X_train_dense = np.asarray(X_train_trans)\n",
    "        \n",
    "    if hasattr(X_test_trans, \"toarray\"):\n",
    "        X_test_dense = X_test_trans.toarray()\n",
    "    else:\n",
    "        X_test_dense = np.asarray(X_test_trans)\n",
    "\n",
    "    # SHAP global importance (mean absolute SHAP)\n",
    "    # --------------------------------------------------------------------\n",
    "    shap_df = None\n",
    "    try:\n",
    "        clf_for_shap = classifier\n",
    "        X_shap_background = X_train_dense[:200] if X_train_dense.shape[0] > 200 else X_train_dense\n",
    "        # choose explainer automatically (TreeExplainer or LinearExplainer or default)\n",
    "        explainer = shap.Explainer(clf_for_shap, X_shap_background, feature_names=feature_names)\n",
    "        shap_values = explainer(X_train_dense[:500])  # compute on subset to save time\n",
    "        vals = shap_values.values\n",
    "        if isinstance(vals, list) or (hasattr(vals, \"dtype\") and vals.dtype == 'object'):\n",
    "            # for classification multiclass shap returns a list per class; average absolute across classes\n",
    "            arrs = [np.abs(v).mean(axis=0) for v in vals]\n",
    "            mean_abs_shap = np.mean(arrs, axis=0)\n",
    "        else:\n",
    "            mean_abs_shap = np.abs(vals).mean(axis=0)\n",
    "        shap_df = pd.DataFrame({\n",
    "            \"feature\": feature_names,\n",
    "            \"shap_mean_abs\": mean_abs_shap\n",
    "        }).sort_values(\"shap_mean_abs\", ascending=False)\n",
    "        print(\"SHAP global importance computed.\")\n",
    "    except Exception as e:\n",
    "        print(\"SHAP global importances failed:\", e)\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # combine importances into a single df for easier comparison \n",
    "    # --------------------------------------------------------------------\n",
    "    imp_combined = pd.DataFrame({\"feature\": feature_names})\n",
    "    if impurity_df is not None:\n",
    "        imp_combined = imp_combined.merge(impurity_df, on=\"feature\", how=\"left\")\n",
    "    if perm_df is not None:\n",
    "        imp_combined = imp_combined.merge(perm_df[['feature','perm_mean']], on=\"feature\", how=\"left\")\n",
    "    if shap_df is not None:\n",
    "        imp_combined = imp_combined.merge(shap_df, on=\"feature\", how=\"left\")\n",
    "\n",
    "    # normalize columns for ranking (0-1) -> so different measures are comparable\n",
    "    for col in ['impurity_importance', 'perm_mean', 'shap_mean_abs']:\n",
    "        if col in imp_combined.columns:\n",
    "            col_vals = imp_combined[col].fillna(0).values\n",
    "            if col_vals.max() > 0:\n",
    "                imp_combined[col + \"_norm\"] = col_vals / col_vals.max()\n",
    "            else:\n",
    "                imp_combined[col + \"_norm\"] = 0.0\n",
    "\n",
    "    # compute a simple aggregated score \n",
    "    norm_cols = [c for c in imp_combined.columns if c.endswith(\"_norm\")]\n",
    "    if len(norm_cols) > 0:\n",
    "        imp_combined['aggregate_score'] = imp_combined[norm_cols].mean(axis=1)\n",
    "        imp_combined = imp_combined.sort_values('aggregate_score', ascending=False)\n",
    "\n",
    "    global_importances_all.append((model_name, imp_combined))\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # local SHAP: 3 representative test instances\n",
    "    # --------------------------------------------------------------------\n",
    "    local_shap_out = None\n",
    "    try:\n",
    "        # compute SHAP values for some test rows -> faster \n",
    "        n_local = min(3, X_test_dense.shape[0])\n",
    "        if shap_df is not None:\n",
    "            local_shap = explainer(X_test_dense[:n_local])\n",
    "            # package local explanations\n",
    "            local_shap_out = {\n",
    "                \"instance_indices\": list(range(n_local)),\n",
    "                \"local_values\": local_shap.values,\n",
    "                \"expected_value\": local_shap.base_values\n",
    "            }\n",
    "            print(f\"Computed local SHAP for {n_local} test samples.\")\n",
    "    except Exception as e:\n",
    "        print(\"Local SHAP failed:\", e)\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # save summary info for table - report \n",
    "    # --------------------------------------------------------------------\n",
    "    summary_rows.append({\n",
    "        \"Algorithms\": model_name,\n",
    "        \"Best parameters (example)\": info[\"best_params\"][0] if info[\"best_params\"] else None,\n",
    "        \"Mean test score\": round(mean_test, 4),\n",
    "        \"Std test score\": round(std_test, 4) if not np.isnan(std_test) else None,\n",
    "        \"Test accuracy (best model)\": round(acc, 4),\n",
    "        \"Test F0.5\": round(f05, 4),\n",
    "        \"Test F1\": round(f1, 4),\n",
    "        \"Test F2\": round(f2, 4),\n",
    "        \"Z_score_above_baseline\": round(z_score, 4) if not np.isnan(z_score) else None,\n",
    "        \"Top feature (aggregate)\": imp_combined['feature'].iloc[0] if 'aggregate_score' in imp_combined.columns and not imp_combined.empty else None,\n",
    "        \"Least feature (aggregate)\": imp_combined['feature'].iloc[-1] if 'aggregate_score' in imp_combined.columns and not imp_combined.empty else None,\n",
    "        \"local_shap_example\": local_shap_out\n",
    "    })\n",
    "\n",
    "# ---------------------------\n",
    "# df summarizing everything\n",
    "# ---------------------------\n",
    "model_perf_df = pd.DataFrame(summary_rows)\n",
    "print(\"\\n\\n=== Model performance summary vs baseline ===\")\n",
    "print(model_perf_df)\n",
    "\n",
    "# ---------------------------\n",
    "# barplot of mean test score with error bars\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(data=model_perf_df, x='Algorithms', y='Mean test score', ci=None)\n",
    "# add error bars manually\n",
    "for i, row in model_perf_df.iterrows():\n",
    "    if pd.notnull(row['Std test score']):\n",
    "        plt.errorbar(i, row['Mean test score'], yerr=row['Std test score'], fmt='none', capsize=5, color='black')\n",
    "plt.axhline(baseline_acc, color='red', linestyle='--', label=f'Baseline acc={baseline_acc:.3f}')\n",
    "plt.ylim(0,1)\n",
    "plt.title(\"Model mean test score (with SD) vs baseline\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# top features across models (aggregate)\n",
    "# --------------------------------------------------------------------\n",
    "# top 10 features per model into a single display\n",
    "top_list = []\n",
    "for model_name, imp_df in global_importances_all:\n",
    "    if 'aggregate_score' in imp_df.columns:\n",
    "        top_feats = imp_df.nlargest(10, 'aggregate_score')[['feature','aggregate_score']].assign(model=model_name)\n",
    "        top_list.append(top_feats)\n",
    "    else:\n",
    "        # fallback to shap or perm\n",
    "        if 'shap_mean_abs' in imp_df.columns:\n",
    "            top_feats = imp_df.nlargest(10, 'shap_mean_abs')[['feature','shap_mean_abs']].assign(model=model_name)\n",
    "            top_list.append(top_feats.rename(columns={'shap_mean_abs':'aggregate_score'}))\n",
    "if len(top_list)>0:\n",
    "    top_feats_all = pd.concat(top_list)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    sns.barplot(data=top_feats_all, x='aggregate_score', y='feature', hue='model')\n",
    "    plt.title(\"Top features per model (top 10 by aggregate score)\")\n",
    "    plt.legend(bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No feature importance data available to plot.\")\n",
    "\n",
    "# ---------------------------\n",
    "# notes so i don't forget\n",
    "# ---------------------------\n",
    "# print(\"\\n\\n=== Suggested Discussion Points ===\")\n",
    "# print(\"- Baseline accuracy (majority class): {:.3f}\".format(baseline_acc))\n",
    "# print(\"- Compare each model's mean test score to baseline; compute z-scores above baseline (table column 'Z_score_above_baseline').\")\n",
    "# print(\"- Models with positive z-scores are performing better than baseline beyond the spread across random states.\")\n",
    "# print(\"- Use permutation importance + SHAP mean-abs + impurity to triangulate which features are truly predictive.\")\n",
    "# print(\"- Unexpected/interesting findings should point to features that are consistently high across all three importance measures or features that differ between models.\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#  FIX SHAP INPUTS (DENSE REQUIRED FOR TREE MODELS)\n",
    "# --------------------------------------------------------------------\n",
    "X_train_dense = X_train_trans.toarray() if hasattr(X_train_trans, \"toarray\") else X_train_trans\n",
    "X_test_dense  = X_test_trans.toarray()  if hasattr(X_test_trans, \"toarray\")  else X_test_trans\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#  SHAP FUNCTION (Robust across model types)\n",
    "# --------------------------------------------------------------------\n",
    "def run_shap(pipeline_model, classifier, model_name, feature_names):\n",
    "\n",
    "    # Transform X_test through pipeline\n",
    "    X_test_trans = pipeline_model.named_steps[\"preprocessor\"].transform(X_test)\n",
    "\n",
    "    # To keep SHAP fast, only first 200 samples\n",
    "    X_shap = X_test_trans[:200]\n",
    "\n",
    "    try:\n",
    "        # --------------------------------------------------------------------\n",
    "        # TREE MODELS -> TreeExplainer\n",
    "        # --------------------------------------------------------------------\n",
    "        if model_name in [\"RandomForest\", \"XGBoost\"]:\n",
    "            explainer = shap.TreeExplainer(classifier)\n",
    "            shap_values = explainer.shap_values(X_shap)\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # LOGISTIC REGRESSION -> KernelExplainer \n",
    "        # --------------------------------------------------------------------\n",
    "        elif model_name == \"LogisticRegression\":\n",
    "            masker = shap.maskers.Independent(X_shap)\n",
    "            explainer = shap.KernelExplainer(classifier.predict_proba, masker)\n",
    "            shap_values = explainer.shap_values(X_shap, nsamples=200)\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # SVM -> SKIP (SHAP Kernel too slow) and crashing my computer \n",
    "        # --------------------------------------------------------------------\n",
    "        elif model_name == \"SVM\":\n",
    "            print(\"Skipping SHAP for SVM — SHAP KernelExplainer is too slow for SVM models.\")\n",
    "            return\n",
    "\n",
    "        else:\n",
    "            print(f\"SHAP not supported for model {model_name}.\")\n",
    "            return\n",
    "\n",
    "        # Make summary plot\n",
    "        shap.summary_plot(\n",
    "            shap_values,\n",
    "            X_shap,\n",
    "            feature_names=feature_names,\n",
    "            show=False\n",
    "        )\n",
    "        plt.title(f\"SHAP Summary Plot – {model_name}\")\n",
    "        plt.show()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"SHAP failed for {model_name}: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#  FEATURE IMPORTANCE FUNCTION\n",
    "# --------------------------------------------------------------------\n",
    "def extract_feature_importance(classifier, model_name, feature_names):\n",
    "    if hasattr(classifier, \"feature_importances_\"):    # RF, XGB\n",
    "        importances = classifier.feature_importances_\n",
    "\n",
    "    elif hasattr(classifier, \"coef_\"):                 # Logistic, Linear SVM\n",
    "        importances = np.abs(classifier.coef_)[0]\n",
    "\n",
    "    else:\n",
    "        print(f\"{model_name} does not support feature importances.\")\n",
    "        return None\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": importances,\n",
    "        \"Model\": model_name\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#  CONFUSION MATRIX + AUROC -> my favs \n",
    "# --------------------------------------------------------------------\n",
    "def run_confusion(pipeline_model, model_name):\n",
    "    preds = pipeline_model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.xlabel(\"Predicted Labels\")   # X-axis label\n",
    "    plt.ylabel(\"True Labels\")        # Y-axis label\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "    plt.title(f\"{model_name} - Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_auroc(pipeline_model, model_name):\n",
    "    if hasattr(pipeline_model, \"predict_proba\"):\n",
    "        probs = pipeline_model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        probs = pipeline_model.decision_function(X_test)\n",
    "\n",
    "    auc = roc_auc_score(y_test, probs)\n",
    "    print(f\"{model_name} AUROC: {auc:.4f}\")\n",
    "    print(classification_report(y_test, pipeline_model.predict(X_test)))\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "#  RUN ANALYSIS FOR ALL MODELS\n",
    "# --------------------------------------------------------------------\n",
    "all_fi = []\n",
    "\n",
    "for model_name, info in results_summary.items():\n",
    "    print(f\"\\n\\n==============================\")\n",
    "    print(f\" FINAL ANALYSIS: {model_name}\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    # Best model (full pipeline)\n",
    "    pipeline_model = info[\"models\"][0]\n",
    "\n",
    "    # Print transformed feature names\n",
    "    try:\n",
    "        feature_names = pipeline_model.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "        print(\"Transformed columns:\", feature_names)\n",
    "    except:\n",
    "        print(\"Raw columns:\", X.columns.tolist())\n",
    "\n",
    "\n",
    "    # Extract classifier component (needed for FI + SHAP)\n",
    "    classifier = pipeline_model.named_steps[\"classifier\"]\n",
    "\n",
    "    # Get processed feature names from the pipeline\n",
    "    feature_names = pipeline_model.named_steps[\"preprocessor\"].get_feature_names_out()\n",
    "\n",
    "    # Confusion matrix and AUROC\n",
    "    run_confusion(pipeline_model, model_name)\n",
    "    run_auroc(pipeline_model, model_name)\n",
    "\n",
    "    # ----------------------------\n",
    "    # FEATURE IMPORTANCE\n",
    "    # ----------------------------\n",
    "    fi_df = extract_feature_importance(\n",
    "        classifier,\n",
    "        model_name=model_name,\n",
    "        feature_names=feature_names\n",
    "    )\n",
    "\n",
    "    if fi_df is not None:\n",
    "        all_fi.append(fi_df)\n",
    "\n",
    "    # ----------------------------\n",
    "    # SHAP\n",
    "    # ----------------------------\n",
    "    run_shap(\n",
    "        pipeline_model=pipeline_model,\n",
    "        classifier=classifier,\n",
    "        model_name=model_name,\n",
    "        feature_names=feature_names\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045fdd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"\\n\\n--------------------------------------------------------------------\")\n",
    "print(\"      EVALUATING BEST MODEL ON NEW UNSEEN DATA -> for fun\")\n",
    "print(\"--------------------------------------------------------------------\\n\")\n",
    "\n",
    "# --------------------------\n",
    "# Load new data (fights not in the kaggle dataset)\n",
    "# --------------------------\n",
    "df_new = pd.read_csv(\"/Users/mallorygo/Desktop/DATA1030/data1030-fall2025/final reports/upcoming_ufc.csv\")    \n",
    "\n",
    "print(\"New dataset shape:\", df_new.shape)\n",
    "print(\"\\nMissing values per column:\\n\", df_new.isna().sum().sort_values(ascending=False))\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit, StratifiedKFold, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import miceforest as mf\n",
    "\n",
    "# ---------------------------\n",
    "# leakage columns\n",
    "# ---------------------------\n",
    "cols_missing = [\n",
    "    'BMatchWCRank', 'RMatchWCRank', 'RWFlyweightRank', 'RWFeatherweightRank',\n",
    "    'RWStrawweightRank', 'RWBantamweightRank', 'RHeavyweightRank',\n",
    "    'RLightHeavyweightRank', 'RMiddleweightRank', 'RWelterweightRank',\n",
    "    'RLightweightRank', 'RFeatherweightRank', 'RBantamweightRank',\n",
    "    'RFlyweightRank', 'RPFPRank', 'BWFlyweightRank', 'BWFeatherweightRank',\n",
    "    'BWStrawweightRank', 'BWBantamweightRank', 'BHeavyweightRank',\n",
    "    'BLightHeavyweightRank', 'BMiddleweightRank', 'BWelterweightRank',\n",
    "    'BLightweightRank', 'BFeatherweightRank', 'BBantamweightRank',\n",
    "    'BFlyweightRank', 'BPFPRank'\n",
    "]\n",
    "\n",
    "cols_leakage = [\n",
    "   'RedAvgSigStrLanded', 'RedAvgSigStrPct', 'RedAvgSubAtt',\n",
    "   'RedAvgTDLanded', 'RedAvgTDPct', 'RedLongestWinStreak',\n",
    "   'RedTotalRoundsFought', 'RedTotalTitleBouts',\n",
    "   'RedWinsByDecisionMajority', 'RedWinsByDecisionSplit', 'RedWinsByDecisionUnanimous',\n",
    "   'RedWinsByKO', 'RedWinsBySubmission', 'RedWinsByTKODoctorStoppage',\n",
    "   'RedWins', 'RedStrikingRatio', 'RedStrikingEfficiency', 'RedTDEfficiency',\n",
    "   'RedEffectiveTD', 'RedExpectedValue', 'RedDecOdds', 'RSubOdds', 'RKOOdds',\n",
    "   'BlueAvgSigStrLanded', 'BlueAvgSigStrPct', 'BlueAvgSubAtt',\n",
    "   'BlueAvgTDLanded', 'BlueAvgTDPct', 'BlueLongestWinStreak',\n",
    "   'BlueTotalRoundsFought', 'BlueTotalTitleBouts',\n",
    "   'BlueWinsByDecisionMajority', 'BlueWinsByDecisionSplit', 'BlueWinsByDecisionUnanimous',\n",
    "   'BlueWinsByKO', 'BlueWinsBySubmission', 'BlueWinsByTKODoctorStoppage',\n",
    "   'BlueWins', 'BlueStrikingRatio', 'BlueStrikingEfficiency', 'BlueTDEfficiency',\n",
    "   'BlueEffectiveTD', 'BlueExpectedValue', 'BlueDecOdds', 'BSubOdds', 'BKOOdds',\n",
    "   'Finish', 'FinishDetails', 'FinishRound', 'FinishRoundTime', 'TotalFightTimeSecs',\n",
    "   'Favorite', 'FavoriteWins', 'EmptyArena',\n",
    "   'SigStrDif', 'AvgSubAttDif', 'AvgTDDif', 'LoseStreakDif', 'WinStreakDif', 'LongestWinStreakDif',\n",
    "   'WinDif', 'LossDif', 'TotalRoundDif', 'TotalTitleBoutDif', 'KODif', 'SubDif',\n",
    "   'NumberOfRounds', 'EffectiveTDDif', 'draw_diff', 'avg_sig_str_pct_diff', 'avg_TD_pct_diff',\n",
    "   'win_by_Decision_Majority_diff', 'win_by_Decision_Split_diff', 'win_by_Decision_Unanimous_diff',\n",
    "   'win_by_TKO_Doctor_Stoppage_dif'\n",
    "]\n",
    "\n",
    "df_clean = df_new.drop(columns=cols_missing + cols_leakage, errors='ignore').copy()\n",
    "\n",
    "# ---------------------------\n",
    "# Drop strongly correlated numeric features (|r| > 0.50)\n",
    "# Strat: For each pair with |r|>0.5 drop the feature with lower variance\n",
    "# ---------------------------\n",
    "num_df_full = df_clean.select_dtypes(include=[np.number])\n",
    "corr_abs = num_df_full.corr().abs()\n",
    "\n",
    "# upper triangle\n",
    "upper = corr_abs.where(np.triu(np.ones(corr_abs.shape), k=1).astype(bool))\n",
    "\n",
    "# pairs with corr > 0.5\n",
    "threshold = 0.50\n",
    "pairs = upper.stack().reset_index()\n",
    "pairs.columns = ['Feature_1', 'Feature_2', 'Correlation']\n",
    "strong_pairs = pairs[pairs['Correlation'] > threshold].copy()\n",
    "\n",
    "print(\"Found strongly correlated pairs (|r| >\", threshold, \"):\", strong_pairs.shape[0])\n",
    "if not strong_pairs.empty:\n",
    "    display(strong_pairs.sort_values('Correlation', ascending=False).head(30))\n",
    "\n",
    "to_drop = set()\n",
    "for f1, f2, corrval in strong_pairs[['Feature_1','Feature_2','Correlation']].values:\n",
    "    if f1 not in num_df_full.columns or f2 not in num_df_full.columns:\n",
    "        continue\n",
    "    v1 = num_df_full[f1].var()\n",
    "    v2 = num_df_full[f2].var()\n",
    "    # if equal variance, drop second to be deterministic\n",
    "    if np.isnan(v1) or np.isnan(v2):\n",
    "        # if variance undefined, drop f2\n",
    "        to_drop.add(f2)\n",
    "    elif v1 < v2:\n",
    "        to_drop.add(f1)\n",
    "    else:\n",
    "        to_drop.add(f2)\n",
    "\n",
    "print(\"Dropping correlated features (one from each pair):\", len(to_drop))\n",
    "if len(to_drop) > 0:\n",
    "    print(sorted(list(to_drop))[:50])\n",
    "df_clean = df_clean.drop(columns=list(to_drop), errors='ignore')\n",
    "\n",
    "# ---------------------------\n",
    "# build X, y\n",
    "# ---------------------------\n",
    "# Ensure Winner exists\n",
    "assert 'Winner' in df_clean.columns, \"Target column 'Winner' not found in df_clean.\"\n",
    "\n",
    "X = df_clean.drop(columns=['Winner']).copy()\n",
    "y = df_clean['Winner'].copy()\n",
    "\n",
    "print(\"\\nAfter correlation drop -> X.shape:\", X.shape)\n",
    "\n",
    "# ---------------------------\n",
    "# Matchup-safe train/test split (grouped)\n",
    "# ---------------------------\n",
    "# build MatchupID\n",
    "df_clean['MatchupID'] = df_clean.apply(\n",
    "    lambda r: \"_\".join(sorted([str(r.get('RedFighter','')), str(r.get('BlueFighter',''))])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# labels encoded for split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_enc = le.fit_transform(y)\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y_enc, groups=df_clean['MatchupID']))\n",
    "\n",
    "X_train = X.iloc[train_idx].reset_index(drop=True)\n",
    "X_test  = X.iloc[test_idx].reset_index(drop=True)\n",
    "y_train = y_enc[train_idx]\n",
    "y_test  = y_enc[test_idx]\n",
    "\n",
    "# drop identity columns from modeling data if they exist\n",
    "for drop_col in ['RedFighter','BlueFighter','MatchupID']:\n",
    "    if drop_col in X_train.columns:\n",
    "        X_train = X_train.drop(columns=[drop_col])\n",
    "        X_test  = X_test.drop(columns=[drop_col])\n",
    "\n",
    "print(\"Train / Test shapes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "# -------------------------\n",
    "# recompute numeric & categorical columns AFTER drops\n",
    "# ---------------------------\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X_train.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "print(\"Numeric columns:\", len(num_cols), \"Categorical columns:\", len(cat_cols))\n",
    "\n",
    "# ---------------------------\n",
    "# MICEFOREST imputation for numeric columns (train+test combined)\n",
    "# ---------------------------\n",
    "# convert to numeric-only frames\n",
    "X_train_num = X_train[num_cols].reset_index(drop=True)\n",
    "X_test_num  = X_test[num_cols].reset_index(drop=True)\n",
    "\n",
    "# make sure columns are strings for miceforest\n",
    "X_train_num.columns = X_train_num.columns.astype(str)\n",
    "X_test_num.columns  = X_test_num.columns.astype(str)\n",
    "\n",
    "X_full_num = pd.concat([X_train_num, X_test_num], axis=0).reset_index(drop=True)\n",
    "\n",
    "kernel = mf.ImputationKernel(\n",
    "    data=X_full_num,\n",
    "    save_all_iterations_data=False,\n",
    "    random_state=42,\n",
    "    num_datasets=1\n",
    ")\n",
    "\n",
    "kernel.mice(iterations=3, n_estimators=50)\n",
    "\n",
    "X_full_completed = kernel.complete_data(0)\n",
    "\n",
    "completed_train = X_full_completed.iloc[:len(X_train)].copy()\n",
    "completed_test  = X_full_completed.iloc[len(X_train):].copy()\n",
    "\n",
    "# Align and write back to X_train/X_test (preserve order)\n",
    "completed_train = completed_train[num_cols]\n",
    "completed_test  = completed_test[num_cols]\n",
    "\n",
    "X_train.loc[:, num_cols] = completed_train.values\n",
    "X_test.loc[:, num_cols]  = completed_test.values\n",
    "\n",
    "print(\"MICE completed; any NaNs left in numeric train?\", X_train[num_cols].isna().sum().sum())\n",
    "\n",
    "#preprocessing \n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', num_transformer, num_cols),\n",
    "    ('cat', cat_transformer, cat_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "# Fit preprocessor on training data \n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Get transformed feature names\n",
    "try:\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "except Exception:\n",
    "    feature_names = list(num_cols) + [f for f in preprocessor.transformers_[1][1].named_steps['onehot'].get_feature_names_out(cat_cols)]\n",
    "\n",
    "print(\"Num transformed features:\", len(feature_names))\n",
    "\n",
    "# ---------------------------\n",
    "# build & train XGBoost pipeline (best model from actual/OG ppeline)\n",
    "# ---------------------------\n",
    "xgb_clf = XGBClassifier(eval_metric='logloss', use_label_encoder=False, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', xgb_clf)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 300],\n",
    "    'classifier__max_depth': [3, 6],\n",
    "    'classifier__learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid = GridSearchCV(pipeline, param_grid, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_pipeline = grid.best_estimator_\n",
    "print(\"Best XGBoost params:\", grid.best_params_)\n",
    "print(\"CV best score:\", grid.best_score_)\n",
    "\n",
    "# ---------------------------\n",
    "# eval on test set\n",
    "# ---------------------------\n",
    "y_test_pred = best_pipeline.predict(X_test)\n",
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "print(\"\\nTest accuracy:\", acc_test)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_test_pred))\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n",
    "\n",
    "# ---------------------------\n",
    "# permutation importance (model-agnostic)\n",
    "# ---------------------------\n",
    "print(\"\\nRunning permutation importance (may take some time)...\")\n",
    "perm = permutation_importance(best_pipeline, X_test, y_test, n_repeats=20, random_state=42, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# assemble perm df aligned with feature_names\n",
    "perm_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"perm_mean\": perm.importances_mean,\n",
    "    \"perm_std\": perm.importances_std\n",
    "}).sort_values('perm_mean', ascending=False)\n",
    "\n",
    "print(\"\\nTop permutation importances:\")\n",
    "display(perm_df.head(20))\n",
    "\n",
    "# ---------------------------\n",
    "# impurity importance (from XGBoost classifier)\n",
    "#     Need to extract classifier after preprocessor -> it's fitted inside pipeline\n",
    "# ---------------------------\n",
    "clf = best_pipeline.named_steps['classifier']\n",
    "# feature_names variable corresponds to transformed features\n",
    "if hasattr(clf, 'feature_importances_'):\n",
    "    imp_df = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"impurity_importance\": clf.feature_importances_\n",
    "    }).sort_values('impurity_importance', ascending=False)\n",
    "    print(\"\\nTop impurity importances (XGBoost):\")\n",
    "    display(imp_df.head(20))\n",
    "else:\n",
    "    print(\"Classifier does not expose feature_importances_\")\n",
    "\n",
    "# ---------------------------\n",
    "# SHAP (TreeExplainer)\n",
    "\n",
    "print(\"\\nComputing SHAP global importance (using TreeExplainer)...\")\n",
    "# transform training data to preprocessed space\n",
    "X_train_trans = best_pipeline.named_steps['preprocessor'].transform(X_train)\n",
    "if hasattr(X_train_trans, \"toarray\"):\n",
    "    X_train_dense = X_train_trans.toarray()\n",
    "else:\n",
    "    X_train_dense = np.asarray(X_train_trans)\n",
    "\n",
    "# choose background subset\n",
    "bg = X_train_dense[:200] if X_train_dense.shape[0] > 200 else X_train_dense\n",
    "\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(clf)\n",
    "    shap_vals = explainer(bg)  # background\n",
    "    # compute shap values for a subset of train for mean-abs\n",
    "    compute_on = X_train_dense[:500] if X_train_dense.shape[0] > 500 else X_train_dense\n",
    "    shap_values_all = explainer(compute_on)\n",
    "    vals = shap_values_all.values\n",
    "    if isinstance(vals, list):  # multiclass returns list\n",
    "        arrs = [np.abs(v).mean(axis=0) for v in vals]\n",
    "        mean_abs_shap = np.mean(arrs, axis=0)\n",
    "    else:\n",
    "        mean_abs_shap = np.abs(vals).mean(axis=0)\n",
    "\n",
    "    shap_df = pd.DataFrame({\n",
    "        \"feature\": feature_names,\n",
    "        \"shap_mean_abs\": mean_abs_shap\n",
    "    }).sort_values('shap_mean_abs', ascending=False)\n",
    "\n",
    "    print(\"\\nTop SHAP mean-abs features:\")\n",
    "    display(shap_df.head(20))\n",
    "\n",
    "    # summary plot (optional)\n",
    "    shap.summary_plot(shap_values_all, compute_on, feature_names=feature_names, show=False)\n",
    "    plt.title(\"SHAP summary (XGBoost)\")\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"SHAP failed:\", e)\n",
    "\n",
    "# ---------------------------\n",
    "# save pipeline (maybe?)\n",
    "# ---------------------------\n",
    "# from joblib import dump\n",
    "# dump(best_pipeline, \"best_xgb_pipeline.joblib\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data1030",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
